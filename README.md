## Краткое описание

Этот проект был реализован в рамках обучения на курсе проффесиональной переподготовки (НИЯУ МИФИ). 
- **Задача:** исследовать непараметрические и параметрические байесовские классификаторы, оценить влияние ширины/типа окна и сравнить с логистической регрессией (Task.pdf).
- **Данные** были сгенерированы из распределения:

!картинка формулы

## Структура ноутбука `название.ipynb`

**Генерация выборки:** формирование классов, визуализация исходных распределений.
!!!Графики

**Проведение разведочного анализа:** построение гистограммы, диаграммы рассеяния, box-диаграммы, оценка
статистических характеристик выборки.

!!! графики

**Обучение непараметрических Байесовских классификаторов:**
  - Расчёт ширины окон по правилу Сильвермана.
  - Кросс-валидация (50 фолдов) для ядер `tophat`, `gaussian`, `epanechnikov`, `linear`.
  - Расчёт средней accuracy и std по train/test.
  - Построение графиков зависимости accuracy и std от коэффициента λ (отношение ширины окна к ширине Сильвермана).
  - Поиск λ с максимальной обобщающей способностью и минимальной дисперсией.
  - Построение диаграмм областей классов для каждого фолда и сводный график со всеми границами.

!!! средние значения

**Метрики качества:** ROC/PR кривые (micro/macro) с AUC для train/test.

!!! Графики

**Обучение параметрического Байеса:** GaussianNB (диагональные ковариационные матрицы), усреднённые accuracy и std.

!!! значения 
**Дополнительные исследования:** 
- Оценка априорных вероятностей.
- Сравнение границ KDE(лучшее ядро) и GaussianNB.
!!! Графики

**Обучение логистической регрессии:** multinomial LogisticRegression, усреднённые accuracy и std.

## Основные результаты

- Лучшее ядро на средней точности по всем λ на тестовой выборке получилось 000.

- Максимальная точность для каждого ядра при его оптимальном λ:
!!!РЕЗУЛЬТАТЫ

Все четыре ядра дают практически одинаковую максимальную точность (≈0.938–0.940) и сопоставимые стандартные отклонения (~0.07), значит метод относительно устойчив к выбору типа ядра. Таким образом, выбор ядра почти не влияет на максимум точности.

- Смещённые априорные вероятности заметно изменяют границы и accuracy. Для данной выборки оптимальнее оставлять априорные вероятности равномерными, т.к. любые искусственные смещения ухудшают обобщающую способность KDE-классификатора.

- GaussianNB уступает непараметрическому классификатору, но обеспечивает плавные границы, логистическая регрессия показывает промежуточный результат.

!!! числа
!!!сравнение моделей график

## Как воспроизвести
1. Открыть `.ipynb` в Jupyter/Colab.
2. Выполнить ячейки по порядку (кросс-валидация и построение графиков может потребовать времени из-за 50 фолдов).
3. Итоговые таблицы и графики автоматически сохраняются в выводах ноутбука.

## Используемые библиотеки
- `numpy`, `pandas`, `matplotlib`
- `scikit-learn` (KDE, GaussianNB, LogisticRegression, метрики)
- `pingouin` (multivariate_normality)
- `itertools`, `collections`, `scipy.special`

